{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook was adapted from Angenent-Mari et al. Their original code can be found here: https://github.com/lrsoenksen/CL_RNA_SynthBio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "# General system libraries\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Math & Visualization Libs\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Import Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import Keras\n",
    "from keras import backend as K\n",
    "from keras.layers import BatchNormalization, Dropout, Flatten, Dense, Input #, Activation, Conv1D, Conv2D, Reshape, Lambda, InputLayer, Convolution2D, MaxPooling1D, MaxPooling2D, ZeroPadding2D, Bidirectional\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Import sklearn libs\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and cross-validating model based on toehold switch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to load desired Toehold dataset file (.csv)\n",
    "data_filename = \"data/mit-data.csv\"\n",
    "#data_filename = \"Sensor-Data.csv\"\n",
    "#data_path = data_folder + data_filename\n",
    "data = pd.read_csv(data_filename)\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed) # Seed can be any number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a random column to test performance and replacing T's with U's in sequences to coorespond to ribosensor data\n",
    "data['random'] = np.random.random(size=len(data))\n",
    "data['seq_SwitchON_GFP'] = data['seq_SwitchON_GFP'].str.replace('T', 'U') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Sequence ID selection\n",
    "#id_data = data['sequence_id']\n",
    "\n",
    "### Toehold Switch dataset input/output columns for selection\n",
    "input_cols = 'seq_SwitchON_GFP'\n",
    "output_cols = ['ON', 'OFF', 'ON_OFF','random']\n",
    "qc_levels = [1.1]\n",
    "doTrain = True\n",
    "loss_init = 'mae' #'logcosh', #'mse', 'mae', 'r2'\n",
    "n_foldCV = 5\n",
    "verbose_init = True\n",
    "evaluate  = True\n",
    "display_init = False\n",
    "\n",
    "### Define data scaler (if any)\n",
    "scaler_init = False\n",
    "scaler = QuantileTransformer(output_distribution='uniform')\n",
    "\n",
    "### DEFINE MODEL NAME (e.g. MLP, CNN, LSTM, etc.)\n",
    "model_name = 'MLP_1D'\n",
    "\n",
    "#Show sample of dataframe structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to pass string DNA/RNA sequence to one-hot\n",
    "def dna2onehot(seq):\n",
    "    #get sequence into an array\n",
    "    seq_array = np.array(list(seq))\n",
    "    \n",
    "    #integer encode the sequence\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded_seq = label_encoder.fit_transform(seq_array)\n",
    "    \n",
    "    #one hot the sequence\n",
    "    onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "    #reshape because that's what OneHotEncoder likes\n",
    "    integer_encoded_seq = integer_encoded_seq.reshape(len(integer_encoded_seq), 1)\n",
    "    onehot_encoded_seq = onehot_encoder.fit_transform(integer_encoded_seq)\n",
    "    \n",
    "    return onehot_encoded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT / OUTPUT DEFINITION, PROCESSING & LOADING\n",
    "#def pre_process_data (data, org_cols, input_cols, output_cols, scaler_init=False, display=True):\n",
    "def pre_process_data (data, input_cols, output_cols, scaler_init=False, display=True):\n",
    "    ## OUTPUT / INPUT DEFINITION, PROCESSING & LOADING\n",
    "    \n",
    "    #Init process bar\n",
    "    tqdm.pandas() # Use `progress_apply` when `applying` one hot encoding and complementarity function to dataframe of input sequences\n",
    "\n",
    "    df_data_input = data[input_cols].dropna()\n",
    "    df_data_input = df_data_input.progress_apply(dna2onehot)\n",
    "    data_input = np.array(list(df_data_input.values))\n",
    "        \n",
    "    # Data Output selection (QC filtered, OutColumns Only & Drop NaNs)\n",
    "    df_data_output= data[output_cols]\n",
    "    df_data_output = df_data_output.dropna(subset=output_cols)\n",
    "    data_output = df_data_output.values.astype('float32')\n",
    "\n",
    "    data_input = data_input[df_data_output.index.values][:][:]\n",
    "\n",
    "    if scaler_init==True:\n",
    "            data_output = scaler.fit_transform(data_output)\n",
    "        \n",
    "    # LOAD FULL LIST OF SEQUENCES after filtering\n",
    "    data_seqs = data[input_cols][df_data_output.index.values]\n",
    "        \n",
    "    # Display processed data if desired\n",
    "    if display==True:\n",
    "        ### Show example of processed dataset\n",
    "        ## Display number of retrieved sequences\n",
    "        print(\"Number of sequences retrieved: \"+str(len(data_input)))\n",
    "        print()\n",
    "\n",
    "        #Select ID to show\n",
    "        toehold_id = 0 \n",
    "\n",
    "        ## Plot Example input toehold matrix \n",
    "        print('EXAMPLE OF INPUT ONE-HOT TOEHOLD')\n",
    "        onehot_encoded_seq = dna2onehot(data_seqs.iloc[toehold_id])\n",
    "        print(onehot_encoded_seq.transpose())\n",
    "        print()\n",
    "        \n",
    "        # Display input size\n",
    "        print(\"Input Size: \" + str(onehot_encoded_seq.shape))\n",
    "        print()\n",
    "\n",
    "        # Display example of Output vector\n",
    "        print('EXAMPLE OF OUTPUT VECTOR')\n",
    "        print(' ' + str(data_output[toehold_id]))\n",
    "        print()\n",
    "\n",
    "    return data_input, data_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to create Keras MLP for regression prediction\n",
    "def create_mlp(width, height, regress=False):\n",
    "    # Define our MLP network\n",
    "    inputShape = (width, height) #DNA/RNA input sequence (one hot encoded)\n",
    "    inputs = Input(shape=inputShape)\n",
    "    chanDim = -1\n",
    "    dropout_init = 0.2\n",
    "    \n",
    "    # Define dense layers\n",
    "    x = inputs\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = BatchNormalization(axis=chanDim)(x)\n",
    "    x = Dropout(dropout_init)(x)\n",
    "    \n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = BatchNormalization(axis=chanDim)(x)\n",
    "    x = Dropout(dropout_init)(x)\n",
    "\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    x = BatchNormalization(axis=chanDim)(x)\n",
    "    x = Dropout(dropout_init)(x)\n",
    "    \n",
    "    # Check to see if the regression node should be added\n",
    "    if regress:\n",
    "        x = Dense(len(output_cols), activation=\"linear\")(x)\n",
    "    else:\n",
    "        x = Dense(len(output_cols), activation=\"sigmoid\")(x)\n",
    "        \n",
    "    # Construct the Model\n",
    "    model = Model(inputs, x) \n",
    "    \n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of R2 metric for testing\n",
    "def r2(x, y):\n",
    "    return stats.pearsonr(x, y)[0] ** 2\n",
    "\n",
    "#Definition of Custom metric as loss related to Coefficient of Determination (R2) \n",
    "#  CoD = 1 - MSE / (variance of inputs), and since this is going to be a loss we want \n",
    "#  improvement to point towards zero, so we choose mse/variance of inputs\n",
    "def custom_r2_loss(y_true, y_pred): \n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return (SS_res/(SS_tot + K.epsilon()))\n",
    "\n",
    "#Definition of Custom metric as loss related to Weigted Mean Absolute error\n",
    "#  Improvement points towards zero, but penalizes loss for small values and improves it for larger values\n",
    "def custom_wmae_loss(y_true, y_pred): \n",
    "    from keras import backend as K\n",
    "    weightedMAE = K.abs((y_true-y_pred)*y_true) #Increase loss for large ON or OFF values -- Skews focus of distribution right\n",
    "    return weightedMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of Coefficient of Determination (R2) for evaluation\n",
    "def cod_r2(x,y):\n",
    "    return r2_score(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define our final model architecture (layers & optimizor) and then compile it\n",
    "def generate_model(trainX, testX, trainY, testY, verbose_init, evaluate=True):\n",
    "    \n",
    "    ## DEEP-LEARNING TRAINING PARAMETERS(e.g. verbose, patients, epoch size, batch size) to constrain process\n",
    "    verbose_init = verbose_init #Zero is no keras verbose\n",
    "    patience_init = 20 # Number of epochs to wait for no model improvement before early stopping a training\n",
    "    epochs_init = 300 # Max number of epochs to perform (to cap training time)\n",
    "    batch_size_init = 64*(1) # number of samples that will be propagated through the network at every epoch dependent on the number of GPUs\n",
    "    validation_spit_init = 0.1 # Percentage of testing data to use in internal validation during training\n",
    "\n",
    "\n",
    "    if verbose_init==True:\n",
    "        # Callback to be used for checkpoint generation and early stopping\n",
    "        callbacks_list = [EarlyStopping(monitor='val_loss', patience=patience_init, verbose = verbose_init)] # Adds Keras integration with TQDM progress bars.\n",
    "    else:\n",
    "        # Callback to be used for checkpoint generation and early stopping\n",
    "        callbacks_list = [EarlyStopping(monitor='val_loss', patience=patience_init, verbose = False)]  \n",
    "    \n",
    "    ## Create Model (Change for MLP, CNN, ETC)\n",
    "    # -------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Define CNN model input shape\n",
    "    (width, height) = np.shape(trainX[0])\n",
    "    \n",
    "    \n",
    "    model = create_mlp(width, height, regress=False)\n",
    "\n",
    "          \n",
    "    ## Initialize the optimizer and Compile model:\n",
    "    #   Custom metric is used (see above), if we use \"Mean absolute percentage error\" that\n",
    "    #   implies that we seek to minimize the absolute percentage difference between \n",
    "    #   our *predictions* and *actual* output values. We also calculate other \n",
    "    #   valuable metrics for regression evaluation \n",
    "    opt = Adam(learning_rate=0.001, epsilon=1e-7, amsgrad=False) # epsilon=1e-1 for POISSON loss\n",
    "    \n",
    "    if loss_init==\"r2\":\n",
    "        model.compile(loss=custom_r2_loss, optimizer=opt,  metrics=['mse','mae', 'mape', 'cosine_similarity', 'acc', custom_r2_loss])\n",
    "    elif loss_init ==\"wmae\":\n",
    "        model.compile(loss=custom_wmae_loss, optimizer=opt,  metrics=['mse','mae', 'mape', 'cosine_similarity', 'acc', custom_wmae_loss])\n",
    "    else:\n",
    "        model.compile(loss=loss_init, optimizer=opt,  metrics=['mse','mae', 'mape', 'cosine_similarity', 'acc']) \n",
    "\n",
    "\n",
    "    model_history = model.fit(trainX, trainY, validation_split=validation_spit_init, epochs=epochs_init, batch_size=batch_size_init, callbacks=callbacks_list, verbose=verbose_init)\n",
    "        \n",
    "    # -------------------------------------------------------------------------------------------------------------------------------------  \n",
    "        \n",
    "       # Init prediction output matrix\n",
    "    testX_Preds = np.zeros_like(testY) #Empty matrix for full prediction evaluation\n",
    "    # Init performance metrics matrix\n",
    "    test_metrics = np.zeros((trainY.shape[1],4)) #Empty matrix for model performance metrics\n",
    "    #test_metrics = np.zeros((len(trainY),3))\n",
    "\n",
    "    # GENERATE PREDICTIONS\n",
    "    if testX.size > 0:\n",
    "        ## Make predictions on testing data:\n",
    "        print(\"Predicting functionality of Test Toeholds ...\")\n",
    "        print(\"\")\n",
    "        #Predictions in scaled space\n",
    "        print(testX_Preds.shape)\n",
    "        testX_Preds = model.predict(testX)\n",
    "        print(testX_Preds.shape)\n",
    "        \n",
    "        if scaler_init == True:\n",
    "            testY = scaler.inverse_transform(testY.reshape(-1, 1))\n",
    "            testX_Preds = scaler.inverse_transform(testX_Preds)\n",
    "        \n",
    "        ## EVALUATE PERFORMANCE OF MODEL\n",
    "        if evaluate==True:\n",
    "            ## Plot training metrics per fold:\n",
    "            plt.figure ()\n",
    "            ax1 = plt.subplot(221)\n",
    "            ax2 = plt.subplot(222)\n",
    "            ax3 = plt.subplot(223)\n",
    "            ax4 = plt.subplot(224)\n",
    "            # Plot MSE metric\n",
    "            ax1.set_title(\"Mean squared error\")\n",
    "            ax1.plot(model_history.history['mse'])\n",
    "            # Plot MAE metric\n",
    "            ax2.set_title(\"Mean absolute error\")\n",
    "            ax2.plot(model_history.history['mae'])\n",
    "            # Plot MAPE metric\n",
    "            ax3.set_title(\"Mean absolute percentage error\")\n",
    "            ax3.plot(model_history.history['mape'])\n",
    "            # Plot CP metric\n",
    "            ax4.set_title(\"Cosine Proximity\")\n",
    "            ax4.plot(model_history.history['cosine_similarity'])\n",
    "            #Tight plot\n",
    "            plt.tight_layout()\n",
    "            \n",
    "\n",
    "            ## Plot compiled training metrics per fold:\n",
    "            plt.figure()\n",
    "            plt.style.use(\"default\")\n",
    "            N = np.arange(0, len(model_history.history[\"loss\"]))\n",
    "            # Plot used Loss metric\n",
    "            plt.plot(N, model_history.history[\"loss\"], label=\"train_loss\")\n",
    "            plt.plot(N, model_history.history[\"val_loss\"], label=\"test_loss\")\n",
    "            # Plot used Accuracy metric (applicable only if categorical model)\n",
    "            plt.plot(N, model_history.history[\"acc\"], label=\"train_acc\")\n",
    "            plt.plot(N, model_history.history[\"val_acc\"], label=\"test_acc\")\n",
    "            # Plot MSE metric\n",
    "            plt.plot(N, model_history.history[\"mse\"], label=\"train_mse\")\n",
    "            plt.plot(N, model_history.history[\"val_mse\"], label=\"test_mse\")\n",
    "            # Plot MAE metric\n",
    "            plt.plot(N, model_history.history[\"mae\"], label=\"train_mae\")\n",
    "            plt.plot(N, model_history.history[\"val_mae\"], label=\"test_mae\")\n",
    "            # Plot MAPE metric\n",
    "            plt.plot(N, model_history.history[\"mape\"], label=\"train_mape\")\n",
    "            plt.plot(N, model_history.history[\"val_mape\"], label=\"test_mape\")\n",
    "            # Plot CP metric\n",
    "            plt.plot(N, model_history.history[\"cosine_similarity\"], label=\"train_cp\")\n",
    "            plt.plot(N, model_history.history[\"val_cosine_similarity\"], label=\"test_cp\")\n",
    "            plt.title(\"CNN Toehold Complementary Rep Data\")\n",
    "            plt.xlabel(\"Epoch #\")\n",
    "            plt.ylabel(\"Loss/Accuracy\")\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.) # Place a legend to the right of this smaller subplot.\n",
    "                        \n",
    "            \n",
    "            ## COMPUTE PERFORMANCE METRICS\n",
    "            # Difference between the *predicted* toehold functionality values and *actual* toehold functionality values, \n",
    "            # then compute the absolute percentage difference for diplay\n",
    "            diff = testX_Preds - testY\n",
    "            abstDiff = np.abs(diff)\n",
    "            # Compute the mean and standard deviation of the absolute difference:\n",
    "            apd_mean = np.mean(abstDiff, axis=0)\n",
    "            apd_std = np.std(abstDiff, axis=0)\n",
    "            apd_r2 = np.zeros_like(apd_mean)\n",
    "            apd_cod_r2 = np.zeros_like(apd_mean)\n",
    "            # Plot: Predicted values vs. Experimental values and get R2 value\n",
    "            for index,item in enumerate(output_cols):\n",
    "\n",
    "                # R2 (Coefficient of Determination)\n",
    "                apd_r2[index] = r2(testX_Preds[:,index], testY[:,index])\n",
    "                apd_cod_r2[index] = cod_r2(testY[:,index], testX_Preds[:,index])\n",
    "                \n",
    "                # Display Output Values\n",
    "                x=np.squeeze(testX_Preds[:,index])\n",
    "                y=np.squeeze(testY[:,index])\n",
    "                \n",
    "                # Display Output Values\n",
    "                print(\"\" + item + \" Mean_absolute_error (TEST): \" + str(apd_mean[index]) + \" (SD: \" + str(apd_std[index]) + \")\" )\n",
    "                print('')\n",
    "                print('EXPERIMENTAL Values vs. PREDICTED values (' + item + ')' )\n",
    "                print('Pearson Correlation: '+ str(stats.pearsonr(x, y)[0]))\n",
    "                print('Spearman Correlation: '+ str(stats.spearmanr(x, y)[0]))\n",
    "                print('R^2: '+ str(apd_r2[index]))\n",
    "                print('Coefficient of Determination (R2): ' + str(apd_cod_r2[index]))\n",
    "                print('')\n",
    "                \n",
    "                # Store model performance metrics for return   \n",
    "                test_metrics[index, :] = [apd_mean[index], apd_std[index], apd_r2[index], apd_cod_r2[index]]\n",
    "                \n",
    "                            \n",
    "    return model, model_history, testX_Preds, test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define our crossvalidation model generator (layers, optimizor, compilation, training, reporting, etc)\n",
    "def generate_crossval_model(X, Y, n_foldCV, verbose_init=True, evaluate=True):\n",
    "    \n",
    "    ## CROSSVALIDATION TRAINING\n",
    "    # Define CV parameters\n",
    "    n_foldCV = n_foldCV #Number of Crossvalidation bins\n",
    "    cv_folds = list(StratifiedKFold(n_splits=n_foldCV, shuffle=True, random_state=seed).split(X,Y.argmax(1))) # Non repeating CV bins\n",
    "    cv_preds = np.zeros_like(Y) #Empty matrix for full prediction evaluation\n",
    "    cv_test_metrics = np.zeros((n_foldCV, Y.shape[1], 4))\n",
    "    deploy_test_metrics = np.zeros((Y.shape[1],4))\n",
    "    \n",
    "    # Perform n-fold crossvalidated training and evaluation\n",
    "    for j, (train_idx, test_idx) in enumerate(cv_folds):\n",
    "        print('\\nFold ',j)\n",
    "        \n",
    "        ## CrossValidation Strategy: \n",
    "        # We use all data for n-crossvalidation this will give us average metrics of performance in future data\n",
    "        # for this all data will be devided into n bins. In every sequential fold we will use n-1 bins for training \n",
    "        # and the remaining bin for testing this split is done in such a way that all data is used for training and\n",
    "        # testing at some point (sweet!). Testing points will be aggregated tenerate an average metric of performance\n",
    "        # and all the datapoints will be put into a master agreement plot for visualization. \n",
    "        # A working model will be made using 75% of the data for training and 25% for further testing.\n",
    "        # A deploy model will be also trained using all available data without testing\n",
    "        trainX_cv = X[train_idx]\n",
    "        trainY_cv = Y[train_idx]\n",
    "        testX_cv = X[test_idx]\n",
    "        testY_cv = Y[test_idx]\n",
    "        # NOTE: Validation set is taken internally from the training set (10% of each fold), this is applied in the the model.fit function\n",
    "        \n",
    "        # Create & Train model each fold according to generator function\n",
    "        model, model_history, testX_Preds, test_metrics = generate_model(trainX_cv, testX_cv, trainY_cv, testY_cv, verbose_init=verbose_init, evaluate=evaluate)\n",
    "        \n",
    "        # Record predicted values of each CV fold training to generate an ensemble reporting\n",
    "        print(\"Predicting functionality of CV-Fold Test Toeholds & Model performance metrics ...\")\n",
    "        cv_preds[test_idx,:] = testX_Preds\n",
    "        cv_test_metrics[j,:,:] = test_metrics\n",
    "        \n",
    "        ## MODEL MEMORY RELEASE\n",
    "        del model_history\n",
    "        del model\n",
    "        for i in range(1): gc.collect()\n",
    "        \n",
    "        ## Free-up keras memmory to prevent leaks\n",
    "        K.clear_session()\n",
    "    \n",
    "    #Transform back data\n",
    "    if scaler_init == True:\n",
    "        Y = scaler.inverse_transform(Y)\n",
    "    \n",
    "    # COMPUTE PERFORMANCE METRICS FOR DEPLOY MODEL\n",
    "    # Difference between the *predicted* toehold functionality values and *actual* toehold functionality values, \n",
    "    # then compute the absolute percentage difference for diplay\n",
    "\n",
    "    diff = cv_preds - Y\n",
    "    abstDiff = np.abs(diff)\n",
    "    # Compute the mean and standard deviation of the absolute difference:\n",
    "    apd_mean = np.mean(abstDiff, axis=0)\n",
    "    apd_std = np.std(abstDiff, axis=0)\n",
    "    apd_r2 = np.zeros_like(apd_mean)\n",
    "    apd_cod_r2 = np.zeros_like(apd_mean)\n",
    "\n",
    "    ## EVALUATE ENSEMBLE CROSSVALIDATION PERFORMANCE OF MODEL\n",
    "    if evaluate==True:\n",
    "        for index,item in enumerate(output_cols): \n",
    "            # R^2 and Coefficient of Determination\n",
    "            apd_r2[index] = r2(cv_preds[:,index], Y[:,index])\n",
    "            apd_cod_r2[index] = cod_r2(Y[:,index], cv_preds[:,index])\n",
    "            # Display Output Values\n",
    "            x_tot=np.squeeze(cv_preds[:,index])\n",
    "            y_tot=np.squeeze(Y[:,index])\n",
    "            print('EXPERIMENTAL Values vs. PREDICTED values (' + item + ')' )\n",
    "            print('Pearson Correlation: '+ str(stats.pearsonr(x_tot, y_tot)[0]))\n",
    "            print('Spearman Correlation: '+ str(stats.spearmanr(x_tot, y_tot)[0]))\n",
    "            print('R^2: '+ str(apd_r2[index]))\n",
    "            print('Coefficient of Determination (R2): ' + str(apd_cod_r2[index]))\n",
    "            print('')\n",
    "            \n",
    "            # Store model performance metrics for return   \n",
    "            deploy_test_metrics[index, :] = [apd_mean[index], apd_std[index], apd_r2[index], apd_cod_r2[index]]\n",
    "            \n",
    "    ## DEPLOYMENT MODEL TRAINING (with full dataset)\n",
    "    # Partition the data into training (80%), validation (10%), testing (10%) splits \n",
    "    (trainX, testX, trainY, testY) = train_test_split(X, Y, test_size=0.1, random_state=seed)\n",
    "    \n",
    "    # Create model function according to generator function\n",
    "    model, model_history, _ , _ = generate_model(trainX, testX, trainY, testY, verbose_init=True, evaluate=True)\n",
    "                                                                                                                                               \n",
    "    ## Return                                     \n",
    "    return model, model_history, cv_preds, cv_test_metrics, deploy_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definer function for full model analysis and reporting\n",
    "def execute_model_analysis(model_name, data, input_cols, output_cols, qc_levels, n_foldCV, verbose_init, evaluate):\n",
    "      \n",
    "    #Iterate through all desired Data QC levels\n",
    "    for j, qc_level in enumerate(qc_levels): \n",
    "            \n",
    "        ## LOAD PREPROCESSED INPUT / OUTPUT\n",
    "        data_input, data_output = pre_process_data (data, input_cols, output_cols, scaler_init=scaler_init, display=display_init)\n",
    "            \n",
    "        ### 3) Model Training using Manual Verification Dataset & Evaluation\n",
    "        # Training with a priori training (75%) & testing (25%) split, with internal training validation from the training set (10% or the 75%)\n",
    "        # This also does valuation on unseen testing data (25%), and saves base model\n",
    "\n",
    "        # Create manual model function according to generator function, train it and display architecture\n",
    "        if doTrain==True:\n",
    "            # Partition the data into training (75%) and testing (25%) splits\n",
    "            (trainX, testX, trainY, testY) = train_test_split(data_input, data_output, test_size=0.25, random_state=seed)\n",
    "            # Generate, Train, Evaluate, Save and Display Model\n",
    "            model, model_history, testX_Preds, test_metrics = generate_model(trainX, testX, trainY, testY, verbose_init=verbose_init, evaluate=evaluate)\n",
    "            model.summary()\n",
    "        \n",
    "            ## MODEL MEMORY RELEASE\n",
    "            del model_history\n",
    "            del model\n",
    "            for i in range(1): gc.collect()\n",
    "        \n",
    "            ## Free-up keras memmory to prevent leaks\n",
    "            K.clear_session()\n",
    "\n",
    "        ### 4) Model Training using k-Fold Cross Validation, Ensemble Evaluation & Full Deployment\n",
    "        # The gold standard for machine learning model evaluation is k-fold cross validation\n",
    "        # It provides a robust estimate of the performance of a model on unseen data. \n",
    "        # It does this by splitting the training dataset into k subsets and takes turns training models on all subsets except one which is held out, and evaluating model performance on the held out validation dataset. The process is repeated until all subsets are given an opportunity to be the held out validation set. \n",
    "        # The performance measure accross all models in the unseen data for each fold\n",
    "        # The performance is printed for each model and it is stored\n",
    "        # A final deployment model trained in all data (no testing) is provided for evaluation in future data\n",
    "\n",
    "        # Create crossvalidated model function according to generator function, train it and display architecture\n",
    "        # Generate, Train, Evaluate, Save and Display Model\n",
    "        if n_foldCV>0:\n",
    "            model, model_history, cv_preds, cv_test_metrics, deploy_test_metrics = generate_crossval_model(data_input, data_output, n_foldCV=n_foldCV, verbose_init=verbose_init, evaluate=evaluate)\n",
    "            model.summary()\n",
    "            \n",
    "            ## MODEL MEMORY RELEASE\n",
    "            del model_history\n",
    "            del model\n",
    "            for i in range(1): gc.collect()\n",
    "                \n",
    "            ## Free-up keras memmory to prevent leaks\n",
    "            K.clear_session()\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN FULL MODEL ANALYSIS AND REPORTING model\n",
    "execute_model_analysis(model_name, data, input_cols, output_cols, qc_levels, n_foldCV, verbose_init, evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on full toehold switch dataset and test on ribosensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to load desired Toehold dataset file (.csv)\n",
    "data2_filename = \"data/mit-data.csv\"\n",
    "data1_filename = \"data/Sensor-Data-Update-9-11-23.csv\"\n",
    "#data_path = data_folder + data_filename\n",
    "df1 = pd.read_csv(data1_filename)\n",
    "df2 = pd.read_csv(data2_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing columns and targets between datasets\n",
    "df2['seq_SwitchON_GFP'] = df2['seq_SwitchON_GFP'].str.replace('T', 'U')\n",
    "df2 = df2[df2['seq_SwitchON_GFP'].notna()]\n",
    "df1['SensorSequence'] = df1['SensorSequence'].str.replace(' ', '') \n",
    "df1['seq_SwitchON_GFP'] = df1['SensorSequence']\n",
    "df1['ON_OFF'] = df1['FoldIncrease']/df1['FoldIncrease'].abs().max()\n",
    "df1['random'] = np.random.random(size=len(df1))\n",
    "df2['random'] = np.random.random(size=len(df2))\n",
    "#df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset input/output columns for selection\n",
    "org_cols = 'seq_SwitchON_GFP'\n",
    "input_cols = 'pad'\n",
    "output_cols = ['ON_OFF', 'random']\n",
    "qc_levels = [1.1]\n",
    "doTrain = True\n",
    "loss_init = 'mae' #'logcosh', #'mse', 'mae', 'r2'\n",
    "n_foldCV = 10\n",
    "verbose_init = True\n",
    "evaluate  = True\n",
    "display_init = False\n",
    "\n",
    "### Define data scaler (if any)\n",
    "scaler_init = False\n",
    "scaler = QuantileTransformer(output_distribution='uniform')\n",
    "\n",
    "### DEFINE MODEL NAME (e.g. MLP, CNN, LSTM, etc.)\n",
    "model_name = 'MLP_1D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_prep import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT / OUTPUT DEFINITION, PROCESSING & LOADING\n",
    "#def pre_process_data (data, org_cols, input_cols, output_cols, scaler_init=False, display=True):\n",
    "def pre_process_data (data, input_cols, output_cols, scaler_init=False, display=True):\n",
    "    ## OUTPUT / INPUT DEFINITION, PROCESSING & LOADING\n",
    "    \n",
    "    #Init process bar\n",
    "    tqdm.pandas() # Use `progress_apply` when `applying` one hot encoding and complementarity function to dataframe of input sequences\n",
    "\n",
    "    #Pad sequences to normalize length\n",
    "    new_df = data.copy()\n",
    "    pad_seq = pad_sequences(data[org_cols], pad_type='end', pad_length=190)\n",
    "    new_df['pad'] = pad_seq\n",
    "\n",
    "    df_data_input = new_df[input_cols].dropna()\n",
    "    df_data_input = df_data_input.progress_apply(dna2onehot)\n",
    "    data_input = np.array(list(df_data_input.values))\n",
    "        \n",
    "    # Data Output selection (QC filtered, OutColumns Only & Drop NaNs)\n",
    "    df_data_output= data[output_cols]\n",
    "    df_data_output = df_data_output.dropna(subset=output_cols)\n",
    "    data_output = df_data_output.values.astype('float32')\n",
    "\n",
    "    data_input = data_input[df_data_output.index.values][:][:]\n",
    "\n",
    "    if scaler_init==True:\n",
    "            data_output = scaler.fit_transform(data_output)\n",
    "        \n",
    "    # LOAD FULL LIST OF SEQUENCES after filtering\n",
    "    data_seqs = new_df[input_cols][df_data_output.index.values]\n",
    "    \n",
    "    # Display processed data if desired\n",
    "    if display==True:\n",
    "        ### Show example of processed dataset\n",
    "        ## Display number of retrieved sequences\n",
    "        print(\"Number of sequences retrieved: \"+str(len(data_input)))\n",
    "        print()\n",
    "\n",
    "        #Select ID to show\n",
    "        toehold_id = 0 \n",
    "\n",
    "        ## Plot Example input toehold matrix \n",
    "        print('EXAMPLE OF INPUT ONE-HOT TOEHOLD')\n",
    "        onehot_encoded_seq = dna2onehot(data_seqs.iloc[toehold_id])\n",
    "        print(onehot_encoded_seq.transpose())\n",
    "        print()\n",
    "\n",
    "        # Display input size\n",
    "        print(\"Input Size: \" + str(onehot_encoded_seq.shape))\n",
    "        print()\n",
    "\n",
    "        # Display example of Output vector\n",
    "        print('EXAMPLE OF OUTPUT VECTOR')\n",
    "        print(' ' + str(data_output[toehold_id]))\n",
    "        print()\n",
    "\n",
    "    return data_input, data_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate model that trains on toehold switch data and tests on ribosensor data\n",
    "trainX, trainY = pre_process_data (df2, input_cols, output_cols, scaler_init=False, display=False)\n",
    "testX, testY = pre_process_data (df1, input_cols, output_cols, scaler_init=False, display=False)\n",
    "model, model_history, testX_Preds, test_metrics = generate_model(trainX, testX, trainY, testY, verbose_init, evaluate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
